{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem Statement:  Implement the backpropagation algorithm from scratch to train a neural network for a simple regression task. You are not allowed to use high-level libraries like TensorFlow or PyTorch for this task. Evaluate the network's performance and explain how the gradients are propagated through the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Weights and biases initialization\n",
    "        self.w1 = np.random.randn(input_size, hidden_size)  # Weights for input to hidden layer\n",
    "        self.b1 = np.zeros((1, hidden_size))  # Biases for hidden layer\n",
    "        self.w2 = np.random.randn(hidden_size, output_size)  # Weights for hidden to output layer\n",
    "        self.b2 = np.zeros((1, output_size))  # Biases for output layer\n",
    "        \n",
    "        # Learning rate\n",
    "        self.learning_rate = 0.01\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.z1 = np.dot(X, self.w1) + self.b1  # Input to hidden layer\n",
    "        self.a1 = self.sigmoid(self.z1)  # Activation in hidden layer\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2  # Hidden to output layer\n",
    "        self.a2 = self.z2  # Linear output for regression task (no activation in output layer)\n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, Y, Y_pred):\n",
    "        # Mean Squared Error (MSE) loss\n",
    "        return np.mean((Y_pred - Y) ** 2)\n",
    "\n",
    "    def backpropagate(self, X, Y):\n",
    "        # Backward pass (backpropagation)\n",
    "        m = Y.shape[0]  # Number of samples\n",
    "\n",
    "        # Calculate the gradients\n",
    "        dL_da2 = 2 * (self.a2 - Y) / m  # Derivative of the loss w.r.t output (MSE gradient)\n",
    "        \n",
    "        # Gradients for weights between hidden and output layer\n",
    "        dL_dw2 = np.dot(self.a1.T, dL_da2)\n",
    "        dL_db2 = np.sum(dL_da2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient for hidden layer activation\n",
    "        dL_da1 = np.dot(dL_da2, self.w2.T) * self.sigmoid_derivative(self.a1)\n",
    "\n",
    "        # Gradients for weights between input and hidden layer\n",
    "        dL_dw1 = np.dot(X.T, dL_da1)\n",
    "        dL_db1 = np.sum(dL_da1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights and biases using gradients\n",
    "        self.w2 -= self.learning_rate * dL_dw2\n",
    "        self.b2 -= self.learning_rate * dL_db2\n",
    "        self.w1 -= self.learning_rate * dL_dw1\n",
    "        self.b1 -= self.learning_rate * dL_db1\n",
    "    \n",
    "    def train(self, X, Y, epochs):\n",
    "        # Train the network\n",
    "        for epoch in range(epochs):\n",
    "            Y_pred = self.forward(X)  # Forward pass\n",
    "            loss = self.compute_loss(Y, Y_pred)  # Compute loss\n",
    "            \n",
    "            self.backpropagate(X, Y)  # Backward pass (backpropagation)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predict output for new inputs\n",
    "        return self.forward(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.5847918757897292\n",
      "Epoch 100, Loss: 0.1563983738299983\n",
      "Epoch 200, Loss: 0.04983915591220746\n",
      "Epoch 300, Loss: 0.04234053764835421\n",
      "Epoch 400, Loss: 0.040748545411325165\n",
      "Epoch 500, Loss: 0.03954843286780654\n",
      "Epoch 600, Loss: 0.03842836145999104\n",
      "Epoch 700, Loss: 0.037367719197446955\n",
      "Epoch 800, Loss: 0.03636175594780193\n",
      "Epoch 900, Loss: 0.035406766007345346\n",
      "Predicted values:\n",
      " [[ 0.4806405 ]\n",
      " [ 2.97051891]\n",
      " [ 2.57931127]\n",
      " [-1.97220613]\n",
      " [ 2.89888564]]\n",
      "Actual values:\n",
      " [[ 0.52879706]\n",
      " [ 2.83116717]\n",
      " [ 2.41001394]\n",
      " [-1.96772864]\n",
      " [ 2.75067768]]\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data for a simple regression task\n",
    "X = np.random.randn(100, 1)  # 100 samples, 1 feature\n",
    "Y = 2 * X + 1  # Linear relation\n",
    "\n",
    "# Initialize and train the network\n",
    "nn = SimpleNeuralNetwork(input_size=1, hidden_size=10, output_size=1)\n",
    "nn.train(X, Y, epochs=1000)\n",
    "\n",
    "# Predict and evaluate performance\n",
    "Y_pred = nn.predict(X)\n",
    "print(\"Predicted values:\\n\", Y_pred[:5])\n",
    "print(\"Actual values:\\n\", Y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
